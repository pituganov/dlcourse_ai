{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50940412e-05, 6.69254912e-03, 9.93262357e-01],\n",
       "       [9.81970011e-01, 1.79854081e-02, 4.45813695e-05]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.softmax(np.array([[-5, 0, 5], [5, 1, -5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([1]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.393354\n",
      "Epoch 1, loss: 2.328966\n",
      "Epoch 2, loss: 2.309817\n",
      "Epoch 3, loss: 2.304124\n",
      "Epoch 4, loss: 2.302426\n",
      "Epoch 5, loss: 2.301922\n",
      "Epoch 6, loss: 2.301769\n",
      "Epoch 7, loss: 2.301727\n",
      "Epoch 8, loss: 2.301713\n",
      "Epoch 9, loss: 2.301708\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x107ae0b70>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKPRFRCJEoS8iEiEKfRGRCFHoi4hEyP8He8YcGmZBINUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106d98c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.116\n",
      "Epoch 0, loss: 2.301707\n",
      "Epoch 1, loss: 2.301706\n",
      "Epoch 2, loss: 2.301709\n",
      "Epoch 3, loss: 2.301707\n",
      "Epoch 4, loss: 2.301709\n",
      "Epoch 5, loss: 2.301708\n",
      "Epoch 6, loss: 2.301707\n",
      "Epoch 7, loss: 2.301709\n",
      "Epoch 8, loss: 2.301708\n",
      "Epoch 9, loss: 2.301707\n",
      "Epoch 10, loss: 2.301706\n",
      "Epoch 11, loss: 2.301707\n",
      "Epoch 12, loss: 2.301707\n",
      "Epoch 13, loss: 2.301708\n",
      "Epoch 14, loss: 2.301708\n",
      "Epoch 15, loss: 2.301708\n",
      "Epoch 16, loss: 2.301706\n",
      "Epoch 17, loss: 2.301709\n",
      "Epoch 18, loss: 2.301707\n",
      "Epoch 19, loss: 2.301709\n",
      "Epoch 20, loss: 2.301707\n",
      "Epoch 21, loss: 2.301707\n",
      "Epoch 22, loss: 2.301707\n",
      "Epoch 23, loss: 2.301708\n",
      "Epoch 24, loss: 2.301707\n",
      "Epoch 25, loss: 2.301710\n",
      "Epoch 26, loss: 2.301709\n",
      "Epoch 27, loss: 2.301708\n",
      "Epoch 28, loss: 2.301706\n",
      "Epoch 29, loss: 2.301708\n",
      "Epoch 30, loss: 2.301708\n",
      "Epoch 31, loss: 2.301710\n",
      "Epoch 32, loss: 2.301707\n",
      "Epoch 33, loss: 2.301709\n",
      "Epoch 34, loss: 2.301707\n",
      "Epoch 35, loss: 2.301709\n",
      "Epoch 36, loss: 2.301708\n",
      "Epoch 37, loss: 2.301709\n",
      "Epoch 38, loss: 2.301706\n",
      "Epoch 39, loss: 2.301707\n",
      "Epoch 40, loss: 2.301708\n",
      "Epoch 41, loss: 2.301708\n",
      "Epoch 42, loss: 2.301706\n",
      "Epoch 43, loss: 2.301711\n",
      "Epoch 44, loss: 2.301707\n",
      "Epoch 45, loss: 2.301707\n",
      "Epoch 46, loss: 2.301707\n",
      "Epoch 47, loss: 2.301711\n",
      "Epoch 48, loss: 2.301710\n",
      "Epoch 49, loss: 2.301707\n",
      "Epoch 50, loss: 2.301709\n",
      "Epoch 51, loss: 2.301708\n",
      "Epoch 52, loss: 2.301707\n",
      "Epoch 53, loss: 2.301710\n",
      "Epoch 54, loss: 2.301711\n",
      "Epoch 55, loss: 2.301710\n",
      "Epoch 56, loss: 2.301709\n",
      "Epoch 57, loss: 2.301711\n",
      "Epoch 58, loss: 2.301713\n",
      "Epoch 59, loss: 2.301711\n",
      "Epoch 60, loss: 2.301707\n",
      "Epoch 61, loss: 2.301709\n",
      "Epoch 62, loss: 2.301709\n",
      "Epoch 63, loss: 2.301707\n",
      "Epoch 64, loss: 2.301711\n",
      "Epoch 65, loss: 2.301707\n",
      "Epoch 66, loss: 2.301708\n",
      "Epoch 67, loss: 2.301706\n",
      "Epoch 68, loss: 2.301711\n",
      "Epoch 69, loss: 2.301707\n",
      "Epoch 70, loss: 2.301708\n",
      "Epoch 71, loss: 2.301708\n",
      "Epoch 72, loss: 2.301707\n",
      "Epoch 73, loss: 2.301706\n",
      "Epoch 74, loss: 2.301709\n",
      "Epoch 75, loss: 2.301707\n",
      "Epoch 76, loss: 2.301708\n",
      "Epoch 77, loss: 2.301709\n",
      "Epoch 78, loss: 2.301707\n",
      "Epoch 79, loss: 2.301708\n",
      "Epoch 80, loss: 2.301709\n",
      "Epoch 81, loss: 2.301707\n",
      "Epoch 82, loss: 2.301707\n",
      "Epoch 83, loss: 2.301707\n",
      "Epoch 84, loss: 2.301708\n",
      "Epoch 85, loss: 2.301707\n",
      "Epoch 86, loss: 2.301708\n",
      "Epoch 87, loss: 2.301711\n",
      "Epoch 88, loss: 2.301707\n",
      "Epoch 89, loss: 2.301708\n",
      "Epoch 90, loss: 2.301711\n",
      "Epoch 91, loss: 2.301708\n",
      "Epoch 92, loss: 2.301710\n",
      "Epoch 93, loss: 2.301707\n",
      "Epoch 94, loss: 2.301707\n",
      "Epoch 95, loss: 2.301710\n",
      "Epoch 96, loss: 2.301708\n",
      "Epoch 97, loss: 2.301710\n",
      "Epoch 98, loss: 2.301707\n",
      "Epoch 99, loss: 2.301706\n",
      "Accuracy after training for 100 epochs:  0.132\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.301231\n",
      "Epoch 1, loss: 2.300241\n",
      "Epoch 2, loss: 2.299288\n",
      "Epoch 3, loss: 2.298363\n",
      "Epoch 4, loss: 2.297457\n",
      "Epoch 5, loss: 2.296569\n",
      "Epoch 6, loss: 2.295692\n",
      "Epoch 7, loss: 2.294828\n",
      "Epoch 8, loss: 2.293972\n",
      "Epoch 9, loss: 2.293124\n",
      "Epoch 10, loss: 2.292286\n",
      "Epoch 11, loss: 2.291455\n",
      "Epoch 12, loss: 2.290630\n",
      "Epoch 13, loss: 2.289813\n",
      "Epoch 14, loss: 2.289003\n",
      "Epoch 15, loss: 2.288200\n",
      "Epoch 16, loss: 2.287403\n",
      "Epoch 17, loss: 2.286612\n",
      "Epoch 18, loss: 2.285828\n",
      "Epoch 19, loss: 2.285050\n",
      "Epoch 20, loss: 2.284278\n",
      "Epoch 21, loss: 2.283512\n",
      "Epoch 22, loss: 2.282752\n",
      "Epoch 23, loss: 2.281998\n",
      "Epoch 24, loss: 2.281249\n",
      "Epoch 25, loss: 2.280506\n",
      "Epoch 26, loss: 2.279769\n",
      "Epoch 27, loss: 2.279037\n",
      "Epoch 28, loss: 2.278310\n",
      "Epoch 29, loss: 2.277589\n",
      "Epoch 30, loss: 2.276873\n",
      "Epoch 31, loss: 2.276162\n",
      "Epoch 32, loss: 2.275456\n",
      "Epoch 33, loss: 2.274756\n",
      "Epoch 34, loss: 2.274060\n",
      "Epoch 35, loss: 2.273369\n",
      "Epoch 36, loss: 2.272683\n",
      "Epoch 37, loss: 2.272002\n",
      "Epoch 38, loss: 2.271326\n",
      "Epoch 39, loss: 2.270654\n",
      "Epoch 40, loss: 2.269987\n",
      "Epoch 41, loss: 2.269324\n",
      "Epoch 42, loss: 2.268666\n",
      "Epoch 43, loss: 2.268012\n",
      "Epoch 44, loss: 2.267363\n",
      "Epoch 45, loss: 2.266718\n",
      "Epoch 46, loss: 2.266077\n",
      "Epoch 47, loss: 2.265441\n",
      "Epoch 48, loss: 2.264809\n",
      "Epoch 49, loss: 2.264181\n",
      "Epoch 50, loss: 2.263557\n",
      "Epoch 51, loss: 2.262937\n",
      "Epoch 52, loss: 2.262321\n",
      "Epoch 53, loss: 2.261709\n",
      "Epoch 54, loss: 2.261101\n",
      "Epoch 55, loss: 2.260497\n",
      "Epoch 56, loss: 2.259897\n",
      "Epoch 57, loss: 2.259301\n",
      "Epoch 58, loss: 2.258709\n",
      "Epoch 59, loss: 2.258120\n",
      "Epoch 60, loss: 2.257535\n",
      "Epoch 61, loss: 2.256953\n",
      "Epoch 62, loss: 2.256375\n",
      "Epoch 63, loss: 2.255801\n",
      "Epoch 64, loss: 2.255231\n",
      "Epoch 65, loss: 2.254663\n",
      "Epoch 66, loss: 2.254100\n",
      "Epoch 67, loss: 2.253540\n",
      "Epoch 68, loss: 2.252983\n",
      "Epoch 69, loss: 2.252429\n",
      "Epoch 70, loss: 2.251880\n",
      "Epoch 71, loss: 2.251333\n",
      "Epoch 72, loss: 2.250790\n",
      "Epoch 73, loss: 2.250250\n",
      "Epoch 74, loss: 2.249714\n",
      "Epoch 75, loss: 2.249180\n",
      "Epoch 76, loss: 2.248650\n",
      "Epoch 77, loss: 2.248123\n",
      "Epoch 78, loss: 2.247599\n",
      "Epoch 79, loss: 2.247078\n",
      "Epoch 80, loss: 2.246561\n",
      "Epoch 81, loss: 2.246046\n",
      "Epoch 82, loss: 2.245535\n",
      "Epoch 83, loss: 2.245026\n",
      "Epoch 84, loss: 2.244521\n",
      "Epoch 85, loss: 2.244018\n",
      "Epoch 86, loss: 2.243519\n",
      "Epoch 87, loss: 2.243022\n",
      "Epoch 88, loss: 2.242529\n",
      "Epoch 89, loss: 2.242038\n",
      "Epoch 90, loss: 2.241550\n",
      "Epoch 91, loss: 2.241065\n",
      "Epoch 92, loss: 2.240582\n",
      "Epoch 93, loss: 2.240103\n",
      "Epoch 94, loss: 2.239626\n",
      "Epoch 95, loss: 2.239152\n",
      "Epoch 96, loss: 2.238681\n",
      "Epoch 97, loss: 2.238212\n",
      "Epoch 98, loss: 2.237746\n",
      "Epoch 99, loss: 2.237283\n",
      "Epoch 100, loss: 2.236823\n",
      "Epoch 101, loss: 2.236365\n",
      "Epoch 102, loss: 2.235910\n",
      "Epoch 103, loss: 2.235457\n",
      "Epoch 104, loss: 2.235007\n",
      "Epoch 105, loss: 2.234559\n",
      "Epoch 106, loss: 2.234115\n",
      "Epoch 107, loss: 2.233672\n",
      "Epoch 108, loss: 2.233232\n",
      "Epoch 109, loss: 2.232795\n",
      "Epoch 110, loss: 2.232360\n",
      "Epoch 111, loss: 2.231928\n",
      "Epoch 112, loss: 2.231497\n",
      "Epoch 113, loss: 2.231070\n",
      "Epoch 114, loss: 2.230645\n",
      "Epoch 115, loss: 2.230222\n",
      "Epoch 116, loss: 2.229801\n",
      "Epoch 117, loss: 2.229383\n",
      "Epoch 118, loss: 2.228967\n",
      "Epoch 119, loss: 2.228554\n",
      "Epoch 120, loss: 2.228143\n",
      "Epoch 121, loss: 2.227734\n",
      "Epoch 122, loss: 2.227327\n",
      "Epoch 123, loss: 2.226923\n",
      "Epoch 124, loss: 2.226521\n",
      "Epoch 125, loss: 2.226121\n",
      "Epoch 126, loss: 2.225723\n",
      "Epoch 127, loss: 2.225328\n",
      "Epoch 128, loss: 2.224935\n",
      "Epoch 129, loss: 2.224544\n",
      "Epoch 130, loss: 2.224155\n",
      "Epoch 131, loss: 2.223768\n",
      "Epoch 132, loss: 2.223383\n",
      "Epoch 133, loss: 2.223001\n",
      "Epoch 134, loss: 2.222620\n",
      "Epoch 135, loss: 2.222242\n",
      "Epoch 136, loss: 2.221865\n",
      "Epoch 137, loss: 2.221491\n",
      "Epoch 138, loss: 2.221119\n",
      "Epoch 139, loss: 2.220749\n",
      "Epoch 140, loss: 2.220381\n",
      "Epoch 141, loss: 2.220014\n",
      "Epoch 142, loss: 2.219650\n",
      "Epoch 143, loss: 2.219288\n",
      "Epoch 144, loss: 2.218928\n",
      "Epoch 145, loss: 2.218569\n",
      "Epoch 146, loss: 2.218213\n",
      "Epoch 147, loss: 2.217858\n",
      "Epoch 148, loss: 2.217505\n",
      "Epoch 149, loss: 2.217155\n",
      "Epoch 150, loss: 2.216806\n",
      "Epoch 151, loss: 2.216459\n",
      "Epoch 152, loss: 2.216113\n",
      "Epoch 153, loss: 2.215770\n",
      "Epoch 154, loss: 2.215429\n",
      "Epoch 155, loss: 2.215089\n",
      "Epoch 156, loss: 2.214751\n",
      "Epoch 157, loss: 2.214416\n",
      "Epoch 158, loss: 2.214081\n",
      "Epoch 159, loss: 2.213749\n",
      "Epoch 160, loss: 2.213418\n",
      "Epoch 161, loss: 2.213089\n",
      "Epoch 162, loss: 2.212762\n",
      "Epoch 163, loss: 2.212436\n",
      "Epoch 164, loss: 2.212112\n",
      "Epoch 165, loss: 2.211790\n",
      "Epoch 166, loss: 2.211470\n",
      "Epoch 167, loss: 2.211151\n",
      "Epoch 168, loss: 2.210834\n",
      "Epoch 169, loss: 2.210519\n",
      "Epoch 170, loss: 2.210205\n",
      "Epoch 171, loss: 2.209893\n",
      "Epoch 172, loss: 2.209582\n",
      "Epoch 173, loss: 2.209273\n",
      "Epoch 174, loss: 2.208966\n",
      "Epoch 175, loss: 2.208660\n",
      "Epoch 176, loss: 2.208356\n",
      "Epoch 177, loss: 2.208054\n",
      "Epoch 178, loss: 2.207753\n",
      "Epoch 179, loss: 2.207453\n",
      "Epoch 180, loss: 2.207155\n",
      "Epoch 181, loss: 2.206859\n",
      "Epoch 182, loss: 2.206564\n",
      "Epoch 183, loss: 2.206271\n",
      "Epoch 184, loss: 2.205979\n",
      "Epoch 185, loss: 2.205689\n",
      "Epoch 186, loss: 2.205400\n",
      "Epoch 187, loss: 2.205112\n",
      "Epoch 188, loss: 2.204826\n",
      "Epoch 189, loss: 2.204542\n",
      "Epoch 190, loss: 2.204259\n",
      "Epoch 191, loss: 2.203977\n",
      "Epoch 192, loss: 2.203697\n",
      "Epoch 193, loss: 2.203418\n",
      "Epoch 194, loss: 2.203141\n",
      "Epoch 195, loss: 2.202865\n",
      "Epoch 196, loss: 2.202590\n",
      "Epoch 197, loss: 2.202317\n",
      "Epoch 198, loss: 2.202045\n",
      "Epoch 199, loss: 2.201774\n",
      "Epoch 0, loss: 2.301405\n",
      "Epoch 1, loss: 2.300410\n",
      "Epoch 2, loss: 2.299452\n",
      "Epoch 3, loss: 2.298523\n",
      "Epoch 4, loss: 2.297611\n",
      "Epoch 5, loss: 2.296718\n",
      "Epoch 6, loss: 2.295839\n",
      "Epoch 7, loss: 2.294971\n",
      "Epoch 8, loss: 2.294114\n",
      "Epoch 9, loss: 2.293265\n",
      "Epoch 10, loss: 2.292425\n",
      "Epoch 11, loss: 2.291592\n",
      "Epoch 12, loss: 2.290766\n",
      "Epoch 13, loss: 2.289947\n",
      "Epoch 14, loss: 2.289136\n",
      "Epoch 15, loss: 2.288331\n",
      "Epoch 16, loss: 2.287533\n",
      "Epoch 17, loss: 2.286741\n",
      "Epoch 18, loss: 2.285956\n",
      "Epoch 19, loss: 2.285176\n",
      "Epoch 20, loss: 2.284403\n",
      "Epoch 21, loss: 2.283636\n",
      "Epoch 22, loss: 2.282875\n",
      "Epoch 23, loss: 2.282120\n",
      "Epoch 24, loss: 2.281370\n",
      "Epoch 25, loss: 2.280626\n",
      "Epoch 26, loss: 2.279888\n",
      "Epoch 27, loss: 2.279155\n",
      "Epoch 28, loss: 2.278427\n",
      "Epoch 29, loss: 2.277705\n",
      "Epoch 30, loss: 2.276988\n",
      "Epoch 31, loss: 2.276276\n",
      "Epoch 32, loss: 2.275570\n",
      "Epoch 33, loss: 2.274868\n",
      "Epoch 34, loss: 2.274172\n",
      "Epoch 35, loss: 2.273480\n",
      "Epoch 36, loss: 2.272793\n",
      "Epoch 37, loss: 2.272111\n",
      "Epoch 38, loss: 2.271434\n",
      "Epoch 39, loss: 2.270762\n",
      "Epoch 40, loss: 2.270094\n",
      "Epoch 41, loss: 2.269430\n",
      "Epoch 42, loss: 2.268772\n",
      "Epoch 43, loss: 2.268117\n",
      "Epoch 44, loss: 2.267467\n",
      "Epoch 45, loss: 2.266821\n",
      "Epoch 46, loss: 2.266180\n",
      "Epoch 47, loss: 2.265543\n",
      "Epoch 48, loss: 2.264910\n",
      "Epoch 49, loss: 2.264281\n",
      "Epoch 50, loss: 2.263657\n",
      "Epoch 51, loss: 2.263037\n",
      "Epoch 52, loss: 2.262420\n",
      "Epoch 53, loss: 2.261808\n",
      "Epoch 54, loss: 2.261199\n",
      "Epoch 55, loss: 2.260594\n",
      "Epoch 56, loss: 2.259993\n",
      "Epoch 57, loss: 2.259396\n",
      "Epoch 58, loss: 2.258803\n",
      "Epoch 59, loss: 2.258214\n",
      "Epoch 60, loss: 2.257628\n",
      "Epoch 61, loss: 2.257046\n",
      "Epoch 62, loss: 2.256468\n",
      "Epoch 63, loss: 2.255893\n",
      "Epoch 64, loss: 2.255322\n",
      "Epoch 65, loss: 2.254754\n",
      "Epoch 66, loss: 2.254190\n",
      "Epoch 67, loss: 2.253629\n",
      "Epoch 68, loss: 2.253072\n",
      "Epoch 69, loss: 2.252518\n",
      "Epoch 70, loss: 2.251968\n",
      "Epoch 71, loss: 2.251421\n",
      "Epoch 72, loss: 2.250877\n",
      "Epoch 73, loss: 2.250336\n",
      "Epoch 74, loss: 2.249799\n",
      "Epoch 75, loss: 2.249265\n",
      "Epoch 76, loss: 2.248734\n",
      "Epoch 77, loss: 2.248207\n",
      "Epoch 78, loss: 2.247682\n",
      "Epoch 79, loss: 2.247161\n",
      "Epoch 80, loss: 2.246643\n",
      "Epoch 81, loss: 2.246127\n",
      "Epoch 82, loss: 2.245615\n",
      "Epoch 83, loss: 2.245106\n",
      "Epoch 84, loss: 2.244600\n",
      "Epoch 85, loss: 2.244097\n",
      "Epoch 86, loss: 2.243597\n",
      "Epoch 87, loss: 2.243100\n",
      "Epoch 88, loss: 2.242605\n",
      "Epoch 89, loss: 2.242114\n",
      "Epoch 90, loss: 2.241625\n",
      "Epoch 91, loss: 2.241140\n",
      "Epoch 92, loss: 2.240657\n",
      "Epoch 93, loss: 2.240177\n",
      "Epoch 94, loss: 2.239699\n",
      "Epoch 95, loss: 2.239225\n",
      "Epoch 96, loss: 2.238753\n",
      "Epoch 97, loss: 2.238284\n",
      "Epoch 98, loss: 2.237817\n",
      "Epoch 99, loss: 2.237354\n",
      "Epoch 100, loss: 2.236893\n",
      "Epoch 101, loss: 2.236434\n",
      "Epoch 102, loss: 2.235978\n",
      "Epoch 103, loss: 2.235525\n",
      "Epoch 104, loss: 2.235075\n",
      "Epoch 105, loss: 2.234627\n",
      "Epoch 106, loss: 2.234181\n",
      "Epoch 107, loss: 2.233738\n",
      "Epoch 108, loss: 2.233298\n",
      "Epoch 109, loss: 2.232859\n",
      "Epoch 110, loss: 2.232424\n",
      "Epoch 111, loss: 2.231991\n",
      "Epoch 112, loss: 2.231560\n",
      "Epoch 113, loss: 2.231132\n",
      "Epoch 114, loss: 2.230706\n",
      "Epoch 115, loss: 2.230283\n",
      "Epoch 116, loss: 2.229861\n",
      "Epoch 117, loss: 2.229443\n",
      "Epoch 118, loss: 2.229026\n",
      "Epoch 119, loss: 2.228612\n",
      "Epoch 120, loss: 2.228200\n",
      "Epoch 121, loss: 2.227791\n",
      "Epoch 122, loss: 2.227384\n",
      "Epoch 123, loss: 2.226979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 2.226576\n",
      "Epoch 125, loss: 2.226175\n",
      "Epoch 126, loss: 2.225777\n",
      "Epoch 127, loss: 2.225381\n",
      "Epoch 128, loss: 2.224987\n",
      "Epoch 129, loss: 2.224595\n",
      "Epoch 130, loss: 2.224206\n",
      "Epoch 131, loss: 2.223819\n",
      "Epoch 132, loss: 2.223433\n",
      "Epoch 133, loss: 2.223050\n",
      "Epoch 134, loss: 2.222669\n",
      "Epoch 135, loss: 2.222290\n",
      "Epoch 136, loss: 2.221912\n",
      "Epoch 137, loss: 2.221538\n",
      "Epoch 138, loss: 2.221165\n",
      "Epoch 139, loss: 2.220794\n",
      "Epoch 140, loss: 2.220425\n",
      "Epoch 141, loss: 2.220058\n",
      "Epoch 142, loss: 2.219693\n",
      "Epoch 143, loss: 2.219330\n",
      "Epoch 144, loss: 2.218969\n",
      "Epoch 145, loss: 2.218610\n",
      "Epoch 146, loss: 2.218253\n",
      "Epoch 147, loss: 2.217898\n",
      "Epoch 148, loss: 2.217544\n",
      "Epoch 149, loss: 2.217193\n",
      "Epoch 150, loss: 2.216844\n",
      "Epoch 151, loss: 2.216496\n",
      "Epoch 152, loss: 2.216150\n",
      "Epoch 153, loss: 2.215806\n",
      "Epoch 154, loss: 2.215464\n",
      "Epoch 155, loss: 2.215124\n",
      "Epoch 156, loss: 2.214786\n",
      "Epoch 157, loss: 2.214449\n",
      "Epoch 158, loss: 2.214115\n",
      "Epoch 159, loss: 2.213782\n",
      "Epoch 160, loss: 2.213450\n",
      "Epoch 161, loss: 2.213120\n",
      "Epoch 162, loss: 2.212792\n",
      "Epoch 163, loss: 2.212465\n",
      "Epoch 164, loss: 2.212141\n",
      "Epoch 165, loss: 2.211818\n",
      "Epoch 166, loss: 2.211497\n",
      "Epoch 167, loss: 2.211178\n",
      "Epoch 168, loss: 2.210860\n",
      "Epoch 169, loss: 2.210544\n",
      "Epoch 170, loss: 2.210229\n",
      "Epoch 171, loss: 2.209916\n",
      "Epoch 172, loss: 2.209605\n",
      "Epoch 173, loss: 2.209296\n",
      "Epoch 174, loss: 2.208988\n",
      "Epoch 175, loss: 2.208681\n",
      "Epoch 176, loss: 2.208376\n",
      "Epoch 177, loss: 2.208073\n",
      "Epoch 178, loss: 2.207772\n",
      "Epoch 179, loss: 2.207471\n",
      "Epoch 180, loss: 2.207173\n",
      "Epoch 181, loss: 2.206875\n",
      "Epoch 182, loss: 2.206580\n",
      "Epoch 183, loss: 2.206286\n",
      "Epoch 184, loss: 2.205993\n",
      "Epoch 185, loss: 2.205702\n",
      "Epoch 186, loss: 2.205413\n",
      "Epoch 187, loss: 2.205124\n",
      "Epoch 188, loss: 2.204838\n",
      "Epoch 189, loss: 2.204553\n",
      "Epoch 190, loss: 2.204269\n",
      "Epoch 191, loss: 2.203987\n",
      "Epoch 192, loss: 2.203706\n",
      "Epoch 193, loss: 2.203426\n",
      "Epoch 194, loss: 2.203148\n",
      "Epoch 195, loss: 2.202871\n",
      "Epoch 196, loss: 2.202596\n",
      "Epoch 197, loss: 2.202322\n",
      "Epoch 198, loss: 2.202049\n",
      "Epoch 199, loss: 2.201778\n",
      "Epoch 0, loss: 2.301487\n",
      "Epoch 1, loss: 2.300483\n",
      "Epoch 2, loss: 2.299521\n",
      "Epoch 3, loss: 2.298588\n",
      "Epoch 4, loss: 2.297680\n",
      "Epoch 5, loss: 2.296789\n",
      "Epoch 6, loss: 2.295911\n",
      "Epoch 7, loss: 2.295045\n",
      "Epoch 8, loss: 2.294188\n",
      "Epoch 9, loss: 2.293341\n",
      "Epoch 10, loss: 2.292502\n",
      "Epoch 11, loss: 2.291671\n",
      "Epoch 12, loss: 2.290846\n",
      "Epoch 13, loss: 2.290030\n",
      "Epoch 14, loss: 2.289220\n",
      "Epoch 15, loss: 2.288416\n",
      "Epoch 16, loss: 2.287619\n",
      "Epoch 17, loss: 2.286829\n",
      "Epoch 18, loss: 2.286044\n",
      "Epoch 19, loss: 2.285266\n",
      "Epoch 20, loss: 2.284494\n",
      "Epoch 21, loss: 2.283728\n",
      "Epoch 22, loss: 2.282967\n",
      "Epoch 23, loss: 2.282213\n",
      "Epoch 24, loss: 2.281463\n",
      "Epoch 25, loss: 2.280720\n",
      "Epoch 26, loss: 2.279982\n",
      "Epoch 27, loss: 2.279250\n",
      "Epoch 28, loss: 2.278523\n",
      "Epoch 29, loss: 2.277801\n",
      "Epoch 30, loss: 2.277084\n",
      "Epoch 31, loss: 2.276372\n",
      "Epoch 32, loss: 2.275666\n",
      "Epoch 33, loss: 2.274964\n",
      "Epoch 34, loss: 2.274268\n",
      "Epoch 35, loss: 2.273576\n",
      "Epoch 36, loss: 2.272889\n",
      "Epoch 37, loss: 2.272207\n",
      "Epoch 38, loss: 2.271529\n",
      "Epoch 39, loss: 2.270857\n",
      "Epoch 40, loss: 2.270188\n",
      "Epoch 41, loss: 2.269525\n",
      "Epoch 42, loss: 2.268865\n",
      "Epoch 43, loss: 2.268211\n",
      "Epoch 44, loss: 2.267560\n",
      "Epoch 45, loss: 2.266914\n",
      "Epoch 46, loss: 2.266272\n",
      "Epoch 47, loss: 2.265635\n",
      "Epoch 48, loss: 2.265001\n",
      "Epoch 49, loss: 2.264372\n",
      "Epoch 50, loss: 2.263747\n",
      "Epoch 51, loss: 2.263126\n",
      "Epoch 52, loss: 2.262509\n",
      "Epoch 53, loss: 2.261895\n",
      "Epoch 54, loss: 2.261286\n",
      "Epoch 55, loss: 2.260681\n",
      "Epoch 56, loss: 2.260079\n",
      "Epoch 57, loss: 2.259481\n",
      "Epoch 58, loss: 2.258888\n",
      "Epoch 59, loss: 2.258297\n",
      "Epoch 60, loss: 2.257711\n",
      "Epoch 61, loss: 2.257128\n",
      "Epoch 62, loss: 2.256549\n",
      "Epoch 63, loss: 2.255973\n",
      "Epoch 64, loss: 2.255401\n",
      "Epoch 65, loss: 2.254832\n",
      "Epoch 66, loss: 2.254267\n",
      "Epoch 67, loss: 2.253706\n",
      "Epoch 68, loss: 2.253147\n",
      "Epoch 69, loss: 2.252593\n",
      "Epoch 70, loss: 2.252041\n",
      "Epoch 71, loss: 2.251493\n",
      "Epoch 72, loss: 2.250949\n",
      "Epoch 73, loss: 2.250407\n",
      "Epoch 74, loss: 2.249869\n",
      "Epoch 75, loss: 2.249334\n",
      "Epoch 76, loss: 2.248802\n",
      "Epoch 77, loss: 2.248274\n",
      "Epoch 78, loss: 2.247748\n",
      "Epoch 79, loss: 2.247226\n",
      "Epoch 80, loss: 2.246707\n",
      "Epoch 81, loss: 2.246191\n",
      "Epoch 82, loss: 2.245678\n",
      "Epoch 83, loss: 2.245168\n",
      "Epoch 84, loss: 2.244661\n",
      "Epoch 85, loss: 2.244157\n",
      "Epoch 86, loss: 2.243655\n",
      "Epoch 87, loss: 2.243157\n",
      "Epoch 88, loss: 2.242662\n",
      "Epoch 89, loss: 2.242170\n",
      "Epoch 90, loss: 2.241680\n",
      "Epoch 91, loss: 2.241193\n",
      "Epoch 92, loss: 2.240710\n",
      "Epoch 93, loss: 2.240229\n",
      "Epoch 94, loss: 2.239750\n",
      "Epoch 95, loss: 2.239275\n",
      "Epoch 96, loss: 2.238802\n",
      "Epoch 97, loss: 2.238332\n",
      "Epoch 98, loss: 2.237864\n",
      "Epoch 99, loss: 2.237400\n",
      "Epoch 100, loss: 2.236938\n",
      "Epoch 101, loss: 2.236478\n",
      "Epoch 102, loss: 2.236022\n",
      "Epoch 103, loss: 2.235568\n",
      "Epoch 104, loss: 2.235116\n",
      "Epoch 105, loss: 2.234667\n",
      "Epoch 106, loss: 2.234221\n",
      "Epoch 107, loss: 2.233777\n",
      "Epoch 108, loss: 2.233335\n",
      "Epoch 109, loss: 2.232896\n",
      "Epoch 110, loss: 2.232460\n",
      "Epoch 111, loss: 2.232026\n",
      "Epoch 112, loss: 2.231594\n",
      "Epoch 113, loss: 2.231165\n",
      "Epoch 114, loss: 2.230738\n",
      "Epoch 115, loss: 2.230314\n",
      "Epoch 116, loss: 2.229892\n",
      "Epoch 117, loss: 2.229473\n",
      "Epoch 118, loss: 2.229055\n",
      "Epoch 119, loss: 2.228641\n",
      "Epoch 120, loss: 2.228228\n",
      "Epoch 121, loss: 2.227818\n",
      "Epoch 122, loss: 2.227410\n",
      "Epoch 123, loss: 2.227004\n",
      "Epoch 124, loss: 2.226600\n",
      "Epoch 125, loss: 2.226199\n",
      "Epoch 126, loss: 2.225800\n",
      "Epoch 127, loss: 2.225403\n",
      "Epoch 128, loss: 2.225008\n",
      "Epoch 129, loss: 2.224615\n",
      "Epoch 130, loss: 2.224225\n",
      "Epoch 131, loss: 2.223837\n",
      "Epoch 132, loss: 2.223451\n",
      "Epoch 133, loss: 2.223067\n",
      "Epoch 134, loss: 2.222685\n",
      "Epoch 135, loss: 2.222305\n",
      "Epoch 136, loss: 2.221927\n",
      "Epoch 137, loss: 2.221551\n",
      "Epoch 138, loss: 2.221178\n",
      "Epoch 139, loss: 2.220806\n",
      "Epoch 140, loss: 2.220437\n",
      "Epoch 141, loss: 2.220069\n",
      "Epoch 142, loss: 2.219704\n",
      "Epoch 143, loss: 2.219340\n",
      "Epoch 144, loss: 2.218978\n",
      "Epoch 145, loss: 2.218618\n",
      "Epoch 146, loss: 2.218260\n",
      "Epoch 147, loss: 2.217905\n",
      "Epoch 148, loss: 2.217550\n",
      "Epoch 149, loss: 2.217198\n",
      "Epoch 150, loss: 2.216848\n",
      "Epoch 151, loss: 2.216500\n",
      "Epoch 152, loss: 2.216153\n",
      "Epoch 153, loss: 2.215809\n",
      "Epoch 154, loss: 2.215466\n",
      "Epoch 155, loss: 2.215125\n",
      "Epoch 156, loss: 2.214786\n",
      "Epoch 157, loss: 2.214448\n",
      "Epoch 158, loss: 2.214113\n",
      "Epoch 159, loss: 2.213779\n",
      "Epoch 160, loss: 2.213447\n",
      "Epoch 161, loss: 2.213117\n",
      "Epoch 162, loss: 2.212788\n",
      "Epoch 163, loss: 2.212461\n",
      "Epoch 164, loss: 2.212136\n",
      "Epoch 165, loss: 2.211813\n",
      "Epoch 166, loss: 2.211491\n",
      "Epoch 167, loss: 2.211171\n",
      "Epoch 168, loss: 2.210853\n",
      "Epoch 169, loss: 2.210536\n",
      "Epoch 170, loss: 2.210221\n",
      "Epoch 171, loss: 2.209907\n",
      "Epoch 172, loss: 2.209596\n",
      "Epoch 173, loss: 2.209286\n",
      "Epoch 174, loss: 2.208977\n",
      "Epoch 175, loss: 2.208670\n",
      "Epoch 176, loss: 2.208365\n",
      "Epoch 177, loss: 2.208061\n",
      "Epoch 178, loss: 2.207759\n",
      "Epoch 179, loss: 2.207458\n",
      "Epoch 180, loss: 2.207159\n",
      "Epoch 181, loss: 2.206861\n",
      "Epoch 182, loss: 2.206565\n",
      "Epoch 183, loss: 2.206270\n",
      "Epoch 184, loss: 2.205977\n",
      "Epoch 185, loss: 2.205686\n",
      "Epoch 186, loss: 2.205395\n",
      "Epoch 187, loss: 2.205107\n",
      "Epoch 188, loss: 2.204820\n",
      "Epoch 189, loss: 2.204534\n",
      "Epoch 190, loss: 2.204250\n",
      "Epoch 191, loss: 2.203967\n",
      "Epoch 192, loss: 2.203685\n",
      "Epoch 193, loss: 2.203405\n",
      "Epoch 194, loss: 2.203127\n",
      "Epoch 195, loss: 2.202849\n",
      "Epoch 196, loss: 2.202574\n",
      "Epoch 197, loss: 2.202299\n",
      "Epoch 198, loss: 2.202026\n",
      "Epoch 199, loss: 2.201754\n",
      "Epoch 0, loss: 2.302803\n",
      "Epoch 1, loss: 2.302684\n",
      "Epoch 2, loss: 2.302566\n",
      "Epoch 3, loss: 2.302450\n",
      "Epoch 4, loss: 2.302335\n",
      "Epoch 5, loss: 2.302220\n",
      "Epoch 6, loss: 2.302107\n",
      "Epoch 7, loss: 2.301995\n",
      "Epoch 8, loss: 2.301884\n",
      "Epoch 9, loss: 2.301773\n",
      "Epoch 10, loss: 2.301664\n",
      "Epoch 11, loss: 2.301555\n",
      "Epoch 12, loss: 2.301447\n",
      "Epoch 13, loss: 2.301340\n",
      "Epoch 14, loss: 2.301234\n",
      "Epoch 15, loss: 2.301129\n",
      "Epoch 16, loss: 2.301024\n",
      "Epoch 17, loss: 2.300920\n",
      "Epoch 18, loss: 2.300816\n",
      "Epoch 19, loss: 2.300713\n",
      "Epoch 20, loss: 2.300611\n",
      "Epoch 21, loss: 2.300509\n",
      "Epoch 22, loss: 2.300408\n",
      "Epoch 23, loss: 2.300307\n",
      "Epoch 24, loss: 2.300207\n",
      "Epoch 25, loss: 2.300108\n",
      "Epoch 26, loss: 2.300009\n",
      "Epoch 27, loss: 2.299910\n",
      "Epoch 28, loss: 2.299812\n",
      "Epoch 29, loss: 2.299714\n",
      "Epoch 30, loss: 2.299617\n",
      "Epoch 31, loss: 2.299520\n",
      "Epoch 32, loss: 2.299423\n",
      "Epoch 33, loss: 2.299327\n",
      "Epoch 34, loss: 2.299232\n",
      "Epoch 35, loss: 2.299136\n",
      "Epoch 36, loss: 2.299041\n",
      "Epoch 37, loss: 2.298946\n",
      "Epoch 38, loss: 2.298852\n",
      "Epoch 39, loss: 2.298758\n",
      "Epoch 40, loss: 2.298664\n",
      "Epoch 41, loss: 2.298571\n",
      "Epoch 42, loss: 2.298477\n",
      "Epoch 43, loss: 2.298385\n",
      "Epoch 44, loss: 2.298292\n",
      "Epoch 45, loss: 2.298199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, loss: 2.298107\n",
      "Epoch 47, loss: 2.298015\n",
      "Epoch 48, loss: 2.297924\n",
      "Epoch 49, loss: 2.297832\n",
      "Epoch 50, loss: 2.297741\n",
      "Epoch 51, loss: 2.297650\n",
      "Epoch 52, loss: 2.297559\n",
      "Epoch 53, loss: 2.297469\n",
      "Epoch 54, loss: 2.297378\n",
      "Epoch 55, loss: 2.297288\n",
      "Epoch 56, loss: 2.297198\n",
      "Epoch 57, loss: 2.297108\n",
      "Epoch 58, loss: 2.297019\n",
      "Epoch 59, loss: 2.296929\n",
      "Epoch 60, loss: 2.296840\n",
      "Epoch 61, loss: 2.296751\n",
      "Epoch 62, loss: 2.296662\n",
      "Epoch 63, loss: 2.296573\n",
      "Epoch 64, loss: 2.296485\n",
      "Epoch 65, loss: 2.296396\n",
      "Epoch 66, loss: 2.296308\n",
      "Epoch 67, loss: 2.296220\n",
      "Epoch 68, loss: 2.296132\n",
      "Epoch 69, loss: 2.296044\n",
      "Epoch 70, loss: 2.295956\n",
      "Epoch 71, loss: 2.295868\n",
      "Epoch 72, loss: 2.295781\n",
      "Epoch 73, loss: 2.295693\n",
      "Epoch 74, loss: 2.295606\n",
      "Epoch 75, loss: 2.295519\n",
      "Epoch 76, loss: 2.295432\n",
      "Epoch 77, loss: 2.295345\n",
      "Epoch 78, loss: 2.295259\n",
      "Epoch 79, loss: 2.295172\n",
      "Epoch 80, loss: 2.295085\n",
      "Epoch 81, loss: 2.294999\n",
      "Epoch 82, loss: 2.294913\n",
      "Epoch 83, loss: 2.294827\n",
      "Epoch 84, loss: 2.294741\n",
      "Epoch 85, loss: 2.294655\n",
      "Epoch 86, loss: 2.294569\n",
      "Epoch 87, loss: 2.294483\n",
      "Epoch 88, loss: 2.294397\n",
      "Epoch 89, loss: 2.294312\n",
      "Epoch 90, loss: 2.294226\n",
      "Epoch 91, loss: 2.294141\n",
      "Epoch 92, loss: 2.294056\n",
      "Epoch 93, loss: 2.293971\n",
      "Epoch 94, loss: 2.293886\n",
      "Epoch 95, loss: 2.293801\n",
      "Epoch 96, loss: 2.293716\n",
      "Epoch 97, loss: 2.293631\n",
      "Epoch 98, loss: 2.293546\n",
      "Epoch 99, loss: 2.293462\n",
      "Epoch 100, loss: 2.293377\n",
      "Epoch 101, loss: 2.293293\n",
      "Epoch 102, loss: 2.293208\n",
      "Epoch 103, loss: 2.293124\n",
      "Epoch 104, loss: 2.293040\n",
      "Epoch 105, loss: 2.292956\n",
      "Epoch 106, loss: 2.292872\n",
      "Epoch 107, loss: 2.292788\n",
      "Epoch 108, loss: 2.292704\n",
      "Epoch 109, loss: 2.292620\n",
      "Epoch 110, loss: 2.292537\n",
      "Epoch 111, loss: 2.292453\n",
      "Epoch 112, loss: 2.292370\n",
      "Epoch 113, loss: 2.292286\n",
      "Epoch 114, loss: 2.292203\n",
      "Epoch 115, loss: 2.292120\n",
      "Epoch 116, loss: 2.292036\n",
      "Epoch 117, loss: 2.291953\n",
      "Epoch 118, loss: 2.291870\n",
      "Epoch 119, loss: 2.291787\n",
      "Epoch 120, loss: 2.291704\n",
      "Epoch 121, loss: 2.291622\n",
      "Epoch 122, loss: 2.291539\n",
      "Epoch 123, loss: 2.291456\n",
      "Epoch 124, loss: 2.291374\n",
      "Epoch 125, loss: 2.291291\n",
      "Epoch 126, loss: 2.291209\n",
      "Epoch 127, loss: 2.291126\n",
      "Epoch 128, loss: 2.291044\n",
      "Epoch 129, loss: 2.290962\n",
      "Epoch 130, loss: 2.290880\n",
      "Epoch 131, loss: 2.290798\n",
      "Epoch 132, loss: 2.290716\n",
      "Epoch 133, loss: 2.290634\n",
      "Epoch 134, loss: 2.290552\n",
      "Epoch 135, loss: 2.290470\n",
      "Epoch 136, loss: 2.290388\n",
      "Epoch 137, loss: 2.290307\n",
      "Epoch 138, loss: 2.290225\n",
      "Epoch 139, loss: 2.290144\n",
      "Epoch 140, loss: 2.290062\n",
      "Epoch 141, loss: 2.289981\n",
      "Epoch 142, loss: 2.289900\n",
      "Epoch 143, loss: 2.289819\n",
      "Epoch 144, loss: 2.289737\n",
      "Epoch 145, loss: 2.289656\n",
      "Epoch 146, loss: 2.289575\n",
      "Epoch 147, loss: 2.289494\n",
      "Epoch 148, loss: 2.289413\n",
      "Epoch 149, loss: 2.289333\n",
      "Epoch 150, loss: 2.289252\n",
      "Epoch 151, loss: 2.289171\n",
      "Epoch 152, loss: 2.289091\n",
      "Epoch 153, loss: 2.289010\n",
      "Epoch 154, loss: 2.288930\n",
      "Epoch 155, loss: 2.288849\n",
      "Epoch 156, loss: 2.288769\n",
      "Epoch 157, loss: 2.288689\n",
      "Epoch 158, loss: 2.288608\n",
      "Epoch 159, loss: 2.288528\n",
      "Epoch 160, loss: 2.288448\n",
      "Epoch 161, loss: 2.288368\n",
      "Epoch 162, loss: 2.288288\n",
      "Epoch 163, loss: 2.288208\n",
      "Epoch 164, loss: 2.288129\n",
      "Epoch 165, loss: 2.288049\n",
      "Epoch 166, loss: 2.287969\n",
      "Epoch 167, loss: 2.287890\n",
      "Epoch 168, loss: 2.287810\n",
      "Epoch 169, loss: 2.287730\n",
      "Epoch 170, loss: 2.287651\n",
      "Epoch 171, loss: 2.287572\n",
      "Epoch 172, loss: 2.287492\n",
      "Epoch 173, loss: 2.287413\n",
      "Epoch 174, loss: 2.287334\n",
      "Epoch 175, loss: 2.287255\n",
      "Epoch 176, loss: 2.287176\n",
      "Epoch 177, loss: 2.287097\n",
      "Epoch 178, loss: 2.287018\n",
      "Epoch 179, loss: 2.286939\n",
      "Epoch 180, loss: 2.286860\n",
      "Epoch 181, loss: 2.286782\n",
      "Epoch 182, loss: 2.286703\n",
      "Epoch 183, loss: 2.286624\n",
      "Epoch 184, loss: 2.286546\n",
      "Epoch 185, loss: 2.286467\n",
      "Epoch 186, loss: 2.286389\n",
      "Epoch 187, loss: 2.286311\n",
      "Epoch 188, loss: 2.286232\n",
      "Epoch 189, loss: 2.286154\n",
      "Epoch 190, loss: 2.286076\n",
      "Epoch 191, loss: 2.285998\n",
      "Epoch 192, loss: 2.285920\n",
      "Epoch 193, loss: 2.285842\n",
      "Epoch 194, loss: 2.285764\n",
      "Epoch 195, loss: 2.285686\n",
      "Epoch 196, loss: 2.285608\n",
      "Epoch 197, loss: 2.285530\n",
      "Epoch 198, loss: 2.285453\n",
      "Epoch 199, loss: 2.285375\n",
      "Epoch 0, loss: 2.302761\n",
      "Epoch 1, loss: 2.302650\n",
      "Epoch 2, loss: 2.302539\n",
      "Epoch 3, loss: 2.302430\n",
      "Epoch 4, loss: 2.302321\n",
      "Epoch 5, loss: 2.302212\n",
      "Epoch 6, loss: 2.302105\n",
      "Epoch 7, loss: 2.301998\n",
      "Epoch 8, loss: 2.301892\n",
      "Epoch 9, loss: 2.301787\n",
      "Epoch 10, loss: 2.301683\n",
      "Epoch 11, loss: 2.301579\n",
      "Epoch 12, loss: 2.301475\n",
      "Epoch 13, loss: 2.301373\n",
      "Epoch 14, loss: 2.301270\n",
      "Epoch 15, loss: 2.301169\n",
      "Epoch 16, loss: 2.301068\n",
      "Epoch 17, loss: 2.300967\n",
      "Epoch 18, loss: 2.300867\n",
      "Epoch 19, loss: 2.300767\n",
      "Epoch 20, loss: 2.300668\n",
      "Epoch 21, loss: 2.300569\n",
      "Epoch 22, loss: 2.300471\n",
      "Epoch 23, loss: 2.300373\n",
      "Epoch 24, loss: 2.300276\n",
      "Epoch 25, loss: 2.300179\n",
      "Epoch 26, loss: 2.300082\n",
      "Epoch 27, loss: 2.299986\n",
      "Epoch 28, loss: 2.299890\n",
      "Epoch 29, loss: 2.299794\n",
      "Epoch 30, loss: 2.299699\n",
      "Epoch 31, loss: 2.299604\n",
      "Epoch 32, loss: 2.299509\n",
      "Epoch 33, loss: 2.299415\n",
      "Epoch 34, loss: 2.299320\n",
      "Epoch 35, loss: 2.299227\n",
      "Epoch 36, loss: 2.299133\n",
      "Epoch 37, loss: 2.299040\n",
      "Epoch 38, loss: 2.298947\n",
      "Epoch 39, loss: 2.298854\n",
      "Epoch 40, loss: 2.298761\n",
      "Epoch 41, loss: 2.298669\n",
      "Epoch 42, loss: 2.298577\n",
      "Epoch 43, loss: 2.298485\n",
      "Epoch 44, loss: 2.298393\n",
      "Epoch 45, loss: 2.298302\n",
      "Epoch 46, loss: 2.298211\n",
      "Epoch 47, loss: 2.298120\n",
      "Epoch 48, loss: 2.298029\n",
      "Epoch 49, loss: 2.297938\n",
      "Epoch 50, loss: 2.297848\n",
      "Epoch 51, loss: 2.297757\n",
      "Epoch 52, loss: 2.297667\n",
      "Epoch 53, loss: 2.297577\n",
      "Epoch 54, loss: 2.297487\n",
      "Epoch 55, loss: 2.297398\n",
      "Epoch 56, loss: 2.297308\n",
      "Epoch 57, loss: 2.297219\n",
      "Epoch 58, loss: 2.297130\n",
      "Epoch 59, loss: 2.297041\n",
      "Epoch 60, loss: 2.296952\n",
      "Epoch 61, loss: 2.296863\n",
      "Epoch 62, loss: 2.296775\n",
      "Epoch 63, loss: 2.296686\n",
      "Epoch 64, loss: 2.296598\n",
      "Epoch 65, loss: 2.296510\n",
      "Epoch 66, loss: 2.296422\n",
      "Epoch 67, loss: 2.296334\n",
      "Epoch 68, loss: 2.296246\n",
      "Epoch 69, loss: 2.296158\n",
      "Epoch 70, loss: 2.296071\n",
      "Epoch 71, loss: 2.295983\n",
      "Epoch 72, loss: 2.295896\n",
      "Epoch 73, loss: 2.295809\n",
      "Epoch 74, loss: 2.295722\n",
      "Epoch 75, loss: 2.295635\n",
      "Epoch 76, loss: 2.295548\n",
      "Epoch 77, loss: 2.295461\n",
      "Epoch 78, loss: 2.295375\n",
      "Epoch 79, loss: 2.295288\n",
      "Epoch 80, loss: 2.295202\n",
      "Epoch 81, loss: 2.295115\n",
      "Epoch 82, loss: 2.295029\n",
      "Epoch 83, loss: 2.294943\n",
      "Epoch 84, loss: 2.294857\n",
      "Epoch 85, loss: 2.294771\n",
      "Epoch 86, loss: 2.294685\n",
      "Epoch 87, loss: 2.294599\n",
      "Epoch 88, loss: 2.294514\n",
      "Epoch 89, loss: 2.294428\n",
      "Epoch 90, loss: 2.294343\n",
      "Epoch 91, loss: 2.294257\n",
      "Epoch 92, loss: 2.294172\n",
      "Epoch 93, loss: 2.294087\n",
      "Epoch 94, loss: 2.294002\n",
      "Epoch 95, loss: 2.293917\n",
      "Epoch 96, loss: 2.293832\n",
      "Epoch 97, loss: 2.293747\n",
      "Epoch 98, loss: 2.293662\n",
      "Epoch 99, loss: 2.293578\n",
      "Epoch 100, loss: 2.293493\n",
      "Epoch 101, loss: 2.293409\n",
      "Epoch 102, loss: 2.293324\n",
      "Epoch 103, loss: 2.293240\n",
      "Epoch 104, loss: 2.293156\n",
      "Epoch 105, loss: 2.293071\n",
      "Epoch 106, loss: 2.292987\n",
      "Epoch 107, loss: 2.292903\n",
      "Epoch 108, loss: 2.292819\n",
      "Epoch 109, loss: 2.292735\n",
      "Epoch 110, loss: 2.292652\n",
      "Epoch 111, loss: 2.292568\n",
      "Epoch 112, loss: 2.292484\n",
      "Epoch 113, loss: 2.292401\n",
      "Epoch 114, loss: 2.292317\n",
      "Epoch 115, loss: 2.292234\n",
      "Epoch 116, loss: 2.292151\n",
      "Epoch 117, loss: 2.292067\n",
      "Epoch 118, loss: 2.291984\n",
      "Epoch 119, loss: 2.291901\n",
      "Epoch 120, loss: 2.291818\n",
      "Epoch 121, loss: 2.291735\n",
      "Epoch 122, loss: 2.291652\n",
      "Epoch 123, loss: 2.291570\n",
      "Epoch 124, loss: 2.291487\n",
      "Epoch 125, loss: 2.291404\n",
      "Epoch 126, loss: 2.291322\n",
      "Epoch 127, loss: 2.291239\n",
      "Epoch 128, loss: 2.291157\n",
      "Epoch 129, loss: 2.291074\n",
      "Epoch 130, loss: 2.290992\n",
      "Epoch 131, loss: 2.290910\n",
      "Epoch 132, loss: 2.290828\n",
      "Epoch 133, loss: 2.290746\n",
      "Epoch 134, loss: 2.290664\n",
      "Epoch 135, loss: 2.290582\n",
      "Epoch 136, loss: 2.290500\n",
      "Epoch 137, loss: 2.290418\n",
      "Epoch 138, loss: 2.290336\n",
      "Epoch 139, loss: 2.290255\n",
      "Epoch 140, loss: 2.290173\n",
      "Epoch 141, loss: 2.290092\n",
      "Epoch 142, loss: 2.290010\n",
      "Epoch 143, loss: 2.289929\n",
      "Epoch 144, loss: 2.289848\n",
      "Epoch 145, loss: 2.289766\n",
      "Epoch 146, loss: 2.289685\n",
      "Epoch 147, loss: 2.289604\n",
      "Epoch 148, loss: 2.289523\n",
      "Epoch 149, loss: 2.289442\n",
      "Epoch 150, loss: 2.289361\n",
      "Epoch 151, loss: 2.289280\n",
      "Epoch 152, loss: 2.289200\n",
      "Epoch 153, loss: 2.289119\n",
      "Epoch 154, loss: 2.289038\n",
      "Epoch 155, loss: 2.288958\n",
      "Epoch 156, loss: 2.288877\n",
      "Epoch 157, loss: 2.288797\n",
      "Epoch 158, loss: 2.288716\n",
      "Epoch 159, loss: 2.288636\n",
      "Epoch 160, loss: 2.288556\n",
      "Epoch 161, loss: 2.288476\n",
      "Epoch 162, loss: 2.288396\n",
      "Epoch 163, loss: 2.288316\n",
      "Epoch 164, loss: 2.288236\n",
      "Epoch 165, loss: 2.288156\n",
      "Epoch 166, loss: 2.288076\n",
      "Epoch 167, loss: 2.287996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168, loss: 2.287916\n",
      "Epoch 169, loss: 2.287837\n",
      "Epoch 170, loss: 2.287757\n",
      "Epoch 171, loss: 2.287678\n",
      "Epoch 172, loss: 2.287598\n",
      "Epoch 173, loss: 2.287519\n",
      "Epoch 174, loss: 2.287440\n",
      "Epoch 175, loss: 2.287360\n",
      "Epoch 176, loss: 2.287281\n",
      "Epoch 177, loss: 2.287202\n",
      "Epoch 178, loss: 2.287123\n",
      "Epoch 179, loss: 2.287044\n",
      "Epoch 180, loss: 2.286965\n",
      "Epoch 181, loss: 2.286886\n",
      "Epoch 182, loss: 2.286807\n",
      "Epoch 183, loss: 2.286728\n",
      "Epoch 184, loss: 2.286650\n",
      "Epoch 185, loss: 2.286571\n",
      "Epoch 186, loss: 2.286493\n",
      "Epoch 187, loss: 2.286414\n",
      "Epoch 188, loss: 2.286336\n",
      "Epoch 189, loss: 2.286257\n",
      "Epoch 190, loss: 2.286179\n",
      "Epoch 191, loss: 2.286101\n",
      "Epoch 192, loss: 2.286022\n",
      "Epoch 193, loss: 2.285944\n",
      "Epoch 194, loss: 2.285866\n",
      "Epoch 195, loss: 2.285788\n",
      "Epoch 196, loss: 2.285710\n",
      "Epoch 197, loss: 2.285632\n",
      "Epoch 198, loss: 2.285554\n",
      "Epoch 199, loss: 2.285477\n",
      "Epoch 0, loss: 2.302781\n",
      "Epoch 1, loss: 2.302664\n",
      "Epoch 2, loss: 2.302549\n",
      "Epoch 3, loss: 2.302434\n",
      "Epoch 4, loss: 2.302321\n",
      "Epoch 5, loss: 2.302208\n",
      "Epoch 6, loss: 2.302097\n",
      "Epoch 7, loss: 2.301986\n",
      "Epoch 8, loss: 2.301876\n",
      "Epoch 9, loss: 2.301768\n",
      "Epoch 10, loss: 2.301660\n",
      "Epoch 11, loss: 2.301552\n",
      "Epoch 12, loss: 2.301446\n",
      "Epoch 13, loss: 2.301340\n",
      "Epoch 14, loss: 2.301235\n",
      "Epoch 15, loss: 2.301130\n",
      "Epoch 16, loss: 2.301026\n",
      "Epoch 17, loss: 2.300923\n",
      "Epoch 18, loss: 2.300821\n",
      "Epoch 19, loss: 2.300719\n",
      "Epoch 20, loss: 2.300617\n",
      "Epoch 21, loss: 2.300516\n",
      "Epoch 22, loss: 2.300416\n",
      "Epoch 23, loss: 2.300316\n",
      "Epoch 24, loss: 2.300216\n",
      "Epoch 25, loss: 2.300117\n",
      "Epoch 26, loss: 2.300019\n",
      "Epoch 27, loss: 2.299920\n",
      "Epoch 28, loss: 2.299823\n",
      "Epoch 29, loss: 2.299726\n",
      "Epoch 30, loss: 2.299629\n",
      "Epoch 31, loss: 2.299532\n",
      "Epoch 32, loss: 2.299436\n",
      "Epoch 33, loss: 2.299340\n",
      "Epoch 34, loss: 2.299245\n",
      "Epoch 35, loss: 2.299149\n",
      "Epoch 36, loss: 2.299055\n",
      "Epoch 37, loss: 2.298960\n",
      "Epoch 38, loss: 2.298866\n",
      "Epoch 39, loss: 2.298772\n",
      "Epoch 40, loss: 2.298678\n",
      "Epoch 41, loss: 2.298585\n",
      "Epoch 42, loss: 2.298492\n",
      "Epoch 43, loss: 2.298399\n",
      "Epoch 44, loss: 2.298307\n",
      "Epoch 45, loss: 2.298214\n",
      "Epoch 46, loss: 2.298122\n",
      "Epoch 47, loss: 2.298030\n",
      "Epoch 48, loss: 2.297939\n",
      "Epoch 49, loss: 2.297847\n",
      "Epoch 50, loss: 2.297756\n",
      "Epoch 51, loss: 2.297665\n",
      "Epoch 52, loss: 2.297574\n",
      "Epoch 53, loss: 2.297484\n",
      "Epoch 54, loss: 2.297393\n",
      "Epoch 55, loss: 2.297303\n",
      "Epoch 56, loss: 2.297213\n",
      "Epoch 57, loss: 2.297123\n",
      "Epoch 58, loss: 2.297033\n",
      "Epoch 59, loss: 2.296944\n",
      "Epoch 60, loss: 2.296854\n",
      "Epoch 61, loss: 2.296765\n",
      "Epoch 62, loss: 2.296676\n",
      "Epoch 63, loss: 2.296587\n",
      "Epoch 64, loss: 2.296498\n",
      "Epoch 65, loss: 2.296410\n",
      "Epoch 66, loss: 2.296321\n",
      "Epoch 67, loss: 2.296233\n",
      "Epoch 68, loss: 2.296145\n",
      "Epoch 69, loss: 2.296057\n",
      "Epoch 70, loss: 2.295969\n",
      "Epoch 71, loss: 2.295881\n",
      "Epoch 72, loss: 2.295793\n",
      "Epoch 73, loss: 2.295706\n",
      "Epoch 74, loss: 2.295618\n",
      "Epoch 75, loss: 2.295531\n",
      "Epoch 76, loss: 2.295444\n",
      "Epoch 77, loss: 2.295357\n",
      "Epoch 78, loss: 2.295270\n",
      "Epoch 79, loss: 2.295183\n",
      "Epoch 80, loss: 2.295096\n",
      "Epoch 81, loss: 2.295010\n",
      "Epoch 82, loss: 2.294923\n",
      "Epoch 83, loss: 2.294837\n",
      "Epoch 84, loss: 2.294750\n",
      "Epoch 85, loss: 2.294664\n",
      "Epoch 86, loss: 2.294578\n",
      "Epoch 87, loss: 2.294492\n",
      "Epoch 88, loss: 2.294406\n",
      "Epoch 89, loss: 2.294320\n",
      "Epoch 90, loss: 2.294235\n",
      "Epoch 91, loss: 2.294149\n",
      "Epoch 92, loss: 2.294064\n",
      "Epoch 93, loss: 2.293978\n",
      "Epoch 94, loss: 2.293893\n",
      "Epoch 95, loss: 2.293808\n",
      "Epoch 96, loss: 2.293723\n",
      "Epoch 97, loss: 2.293638\n",
      "Epoch 98, loss: 2.293553\n",
      "Epoch 99, loss: 2.293468\n",
      "Epoch 100, loss: 2.293383\n",
      "Epoch 101, loss: 2.293298\n",
      "Epoch 102, loss: 2.293214\n",
      "Epoch 103, loss: 2.293129\n",
      "Epoch 104, loss: 2.293045\n",
      "Epoch 105, loss: 2.292961\n",
      "Epoch 106, loss: 2.292876\n",
      "Epoch 107, loss: 2.292792\n",
      "Epoch 108, loss: 2.292708\n",
      "Epoch 109, loss: 2.292624\n",
      "Epoch 110, loss: 2.292540\n",
      "Epoch 111, loss: 2.292456\n",
      "Epoch 112, loss: 2.292373\n",
      "Epoch 113, loss: 2.292289\n",
      "Epoch 114, loss: 2.292205\n",
      "Epoch 115, loss: 2.292122\n",
      "Epoch 116, loss: 2.292038\n",
      "Epoch 117, loss: 2.291955\n",
      "Epoch 118, loss: 2.291872\n",
      "Epoch 119, loss: 2.291788\n",
      "Epoch 120, loss: 2.291705\n",
      "Epoch 121, loss: 2.291622\n",
      "Epoch 122, loss: 2.291539\n",
      "Epoch 123, loss: 2.291456\n",
      "Epoch 124, loss: 2.291374\n",
      "Epoch 125, loss: 2.291291\n",
      "Epoch 126, loss: 2.291208\n",
      "Epoch 127, loss: 2.291125\n",
      "Epoch 128, loss: 2.291043\n",
      "Epoch 129, loss: 2.290960\n",
      "Epoch 130, loss: 2.290878\n",
      "Epoch 131, loss: 2.290796\n",
      "Epoch 132, loss: 2.290714\n",
      "Epoch 133, loss: 2.290631\n",
      "Epoch 134, loss: 2.290549\n",
      "Epoch 135, loss: 2.290467\n",
      "Epoch 136, loss: 2.290385\n",
      "Epoch 137, loss: 2.290303\n",
      "Epoch 138, loss: 2.290222\n",
      "Epoch 139, loss: 2.290140\n",
      "Epoch 140, loss: 2.290058\n",
      "Epoch 141, loss: 2.289977\n",
      "Epoch 142, loss: 2.289895\n",
      "Epoch 143, loss: 2.289814\n",
      "Epoch 144, loss: 2.289732\n",
      "Epoch 145, loss: 2.289651\n",
      "Epoch 146, loss: 2.289570\n",
      "Epoch 147, loss: 2.289488\n",
      "Epoch 148, loss: 2.289407\n",
      "Epoch 149, loss: 2.289326\n",
      "Epoch 150, loss: 2.289245\n",
      "Epoch 151, loss: 2.289164\n",
      "Epoch 152, loss: 2.289083\n",
      "Epoch 153, loss: 2.289003\n",
      "Epoch 154, loss: 2.288922\n",
      "Epoch 155, loss: 2.288841\n",
      "Epoch 156, loss: 2.288761\n",
      "Epoch 157, loss: 2.288680\n",
      "Epoch 158, loss: 2.288600\n",
      "Epoch 159, loss: 2.288519\n",
      "Epoch 160, loss: 2.288439\n",
      "Epoch 161, loss: 2.288359\n",
      "Epoch 162, loss: 2.288279\n",
      "Epoch 163, loss: 2.288198\n",
      "Epoch 164, loss: 2.288118\n",
      "Epoch 165, loss: 2.288038\n",
      "Epoch 166, loss: 2.287958\n",
      "Epoch 167, loss: 2.287879\n",
      "Epoch 168, loss: 2.287799\n",
      "Epoch 169, loss: 2.287719\n",
      "Epoch 170, loss: 2.287639\n",
      "Epoch 171, loss: 2.287560\n",
      "Epoch 172, loss: 2.287480\n",
      "Epoch 173, loss: 2.287401\n",
      "Epoch 174, loss: 2.287321\n",
      "Epoch 175, loss: 2.287242\n",
      "Epoch 176, loss: 2.287163\n",
      "Epoch 177, loss: 2.287084\n",
      "Epoch 178, loss: 2.287004\n",
      "Epoch 179, loss: 2.286925\n",
      "Epoch 180, loss: 2.286846\n",
      "Epoch 181, loss: 2.286767\n",
      "Epoch 182, loss: 2.286689\n",
      "Epoch 183, loss: 2.286610\n",
      "Epoch 184, loss: 2.286531\n",
      "Epoch 185, loss: 2.286452\n",
      "Epoch 186, loss: 2.286374\n",
      "Epoch 187, loss: 2.286295\n",
      "Epoch 188, loss: 2.286217\n",
      "Epoch 189, loss: 2.286138\n",
      "Epoch 190, loss: 2.286060\n",
      "Epoch 191, loss: 2.285981\n",
      "Epoch 192, loss: 2.285903\n",
      "Epoch 193, loss: 2.285825\n",
      "Epoch 194, loss: 2.285747\n",
      "Epoch 195, loss: 2.285669\n",
      "Epoch 196, loss: 2.285591\n",
      "Epoch 197, loss: 2.285513\n",
      "Epoch 198, loss: 2.285435\n",
      "Epoch 199, loss: 2.285357\n",
      "Epoch 0, loss: 2.302437\n",
      "Epoch 1, loss: 2.302425\n",
      "Epoch 2, loss: 2.302414\n",
      "Epoch 3, loss: 2.302402\n",
      "Epoch 4, loss: 2.302391\n",
      "Epoch 5, loss: 2.302379\n",
      "Epoch 6, loss: 2.302368\n",
      "Epoch 7, loss: 2.302356\n",
      "Epoch 8, loss: 2.302345\n",
      "Epoch 9, loss: 2.302333\n",
      "Epoch 10, loss: 2.302322\n",
      "Epoch 11, loss: 2.302310\n",
      "Epoch 12, loss: 2.302299\n",
      "Epoch 13, loss: 2.302287\n",
      "Epoch 14, loss: 2.302276\n",
      "Epoch 15, loss: 2.302264\n",
      "Epoch 16, loss: 2.302253\n",
      "Epoch 17, loss: 2.302242\n",
      "Epoch 18, loss: 2.302230\n",
      "Epoch 19, loss: 2.302219\n",
      "Epoch 20, loss: 2.302207\n",
      "Epoch 21, loss: 2.302196\n",
      "Epoch 22, loss: 2.302185\n",
      "Epoch 23, loss: 2.302173\n",
      "Epoch 24, loss: 2.302162\n",
      "Epoch 25, loss: 2.302151\n",
      "Epoch 26, loss: 2.302139\n",
      "Epoch 27, loss: 2.302128\n",
      "Epoch 28, loss: 2.302117\n",
      "Epoch 29, loss: 2.302105\n",
      "Epoch 30, loss: 2.302094\n",
      "Epoch 31, loss: 2.302083\n",
      "Epoch 32, loss: 2.302072\n",
      "Epoch 33, loss: 2.302060\n",
      "Epoch 34, loss: 2.302049\n",
      "Epoch 35, loss: 2.302038\n",
      "Epoch 36, loss: 2.302027\n",
      "Epoch 37, loss: 2.302015\n",
      "Epoch 38, loss: 2.302004\n",
      "Epoch 39, loss: 2.301993\n",
      "Epoch 40, loss: 2.301982\n",
      "Epoch 41, loss: 2.301971\n",
      "Epoch 42, loss: 2.301960\n",
      "Epoch 43, loss: 2.301948\n",
      "Epoch 44, loss: 2.301937\n",
      "Epoch 45, loss: 2.301926\n",
      "Epoch 46, loss: 2.301915\n",
      "Epoch 47, loss: 2.301904\n",
      "Epoch 48, loss: 2.301893\n",
      "Epoch 49, loss: 2.301882\n",
      "Epoch 50, loss: 2.301871\n",
      "Epoch 51, loss: 2.301859\n",
      "Epoch 52, loss: 2.301848\n",
      "Epoch 53, loss: 2.301837\n",
      "Epoch 54, loss: 2.301826\n",
      "Epoch 55, loss: 2.301815\n",
      "Epoch 56, loss: 2.301804\n",
      "Epoch 57, loss: 2.301793\n",
      "Epoch 58, loss: 2.301782\n",
      "Epoch 59, loss: 2.301771\n",
      "Epoch 60, loss: 2.301760\n",
      "Epoch 61, loss: 2.301749\n",
      "Epoch 62, loss: 2.301738\n",
      "Epoch 63, loss: 2.301727\n",
      "Epoch 64, loss: 2.301716\n",
      "Epoch 65, loss: 2.301705\n",
      "Epoch 66, loss: 2.301694\n",
      "Epoch 67, loss: 2.301683\n",
      "Epoch 68, loss: 2.301672\n",
      "Epoch 69, loss: 2.301662\n",
      "Epoch 70, loss: 2.301651\n",
      "Epoch 71, loss: 2.301640\n",
      "Epoch 72, loss: 2.301629\n",
      "Epoch 73, loss: 2.301618\n",
      "Epoch 74, loss: 2.301607\n",
      "Epoch 75, loss: 2.301596\n",
      "Epoch 76, loss: 2.301585\n",
      "Epoch 77, loss: 2.301574\n",
      "Epoch 78, loss: 2.301564\n",
      "Epoch 79, loss: 2.301553\n",
      "Epoch 80, loss: 2.301542\n",
      "Epoch 81, loss: 2.301531\n",
      "Epoch 82, loss: 2.301520\n",
      "Epoch 83, loss: 2.301509\n",
      "Epoch 84, loss: 2.301499\n",
      "Epoch 85, loss: 2.301488\n",
      "Epoch 86, loss: 2.301477\n",
      "Epoch 87, loss: 2.301466\n",
      "Epoch 88, loss: 2.301456\n",
      "Epoch 89, loss: 2.301445\n",
      "Epoch 90, loss: 2.301434\n",
      "Epoch 91, loss: 2.301423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss: 2.301413\n",
      "Epoch 93, loss: 2.301402\n",
      "Epoch 94, loss: 2.301391\n",
      "Epoch 95, loss: 2.301380\n",
      "Epoch 96, loss: 2.301370\n",
      "Epoch 97, loss: 2.301359\n",
      "Epoch 98, loss: 2.301348\n",
      "Epoch 99, loss: 2.301338\n",
      "Epoch 100, loss: 2.301327\n",
      "Epoch 101, loss: 2.301316\n",
      "Epoch 102, loss: 2.301306\n",
      "Epoch 103, loss: 2.301295\n",
      "Epoch 104, loss: 2.301284\n",
      "Epoch 105, loss: 2.301274\n",
      "Epoch 106, loss: 2.301263\n",
      "Epoch 107, loss: 2.301253\n",
      "Epoch 108, loss: 2.301242\n",
      "Epoch 109, loss: 2.301231\n",
      "Epoch 110, loss: 2.301221\n",
      "Epoch 111, loss: 2.301210\n",
      "Epoch 112, loss: 2.301200\n",
      "Epoch 113, loss: 2.301189\n",
      "Epoch 114, loss: 2.301178\n",
      "Epoch 115, loss: 2.301168\n",
      "Epoch 116, loss: 2.301157\n",
      "Epoch 117, loss: 2.301147\n",
      "Epoch 118, loss: 2.301136\n",
      "Epoch 119, loss: 2.301126\n",
      "Epoch 120, loss: 2.301115\n",
      "Epoch 121, loss: 2.301105\n",
      "Epoch 122, loss: 2.301094\n",
      "Epoch 123, loss: 2.301084\n",
      "Epoch 124, loss: 2.301073\n",
      "Epoch 125, loss: 2.301063\n",
      "Epoch 126, loss: 2.301052\n",
      "Epoch 127, loss: 2.301042\n",
      "Epoch 128, loss: 2.301031\n",
      "Epoch 129, loss: 2.301021\n",
      "Epoch 130, loss: 2.301010\n",
      "Epoch 131, loss: 2.301000\n",
      "Epoch 132, loss: 2.300989\n",
      "Epoch 133, loss: 2.300979\n",
      "Epoch 134, loss: 2.300968\n",
      "Epoch 135, loss: 2.300958\n",
      "Epoch 136, loss: 2.300948\n",
      "Epoch 137, loss: 2.300937\n",
      "Epoch 138, loss: 2.300927\n",
      "Epoch 139, loss: 2.300916\n",
      "Epoch 140, loss: 2.300906\n",
      "Epoch 141, loss: 2.300896\n",
      "Epoch 142, loss: 2.300885\n",
      "Epoch 143, loss: 2.300875\n",
      "Epoch 144, loss: 2.300865\n",
      "Epoch 145, loss: 2.300854\n",
      "Epoch 146, loss: 2.300844\n",
      "Epoch 147, loss: 2.300833\n",
      "Epoch 148, loss: 2.300823\n",
      "Epoch 149, loss: 2.300813\n",
      "Epoch 150, loss: 2.300802\n",
      "Epoch 151, loss: 2.300792\n",
      "Epoch 152, loss: 2.300782\n",
      "Epoch 153, loss: 2.300772\n",
      "Epoch 154, loss: 2.300761\n",
      "Epoch 155, loss: 2.300751\n",
      "Epoch 156, loss: 2.300741\n",
      "Epoch 157, loss: 2.300730\n",
      "Epoch 158, loss: 2.300720\n",
      "Epoch 159, loss: 2.300710\n",
      "Epoch 160, loss: 2.300700\n",
      "Epoch 161, loss: 2.300689\n",
      "Epoch 162, loss: 2.300679\n",
      "Epoch 163, loss: 2.300669\n",
      "Epoch 164, loss: 2.300659\n",
      "Epoch 165, loss: 2.300648\n",
      "Epoch 166, loss: 2.300638\n",
      "Epoch 167, loss: 2.300628\n",
      "Epoch 168, loss: 2.300618\n",
      "Epoch 169, loss: 2.300607\n",
      "Epoch 170, loss: 2.300597\n",
      "Epoch 171, loss: 2.300587\n",
      "Epoch 172, loss: 2.300577\n",
      "Epoch 173, loss: 2.300567\n",
      "Epoch 174, loss: 2.300556\n",
      "Epoch 175, loss: 2.300546\n",
      "Epoch 176, loss: 2.300536\n",
      "Epoch 177, loss: 2.300526\n",
      "Epoch 178, loss: 2.300516\n",
      "Epoch 179, loss: 2.300506\n",
      "Epoch 180, loss: 2.300495\n",
      "Epoch 181, loss: 2.300485\n",
      "Epoch 182, loss: 2.300475\n",
      "Epoch 183, loss: 2.300465\n",
      "Epoch 184, loss: 2.300455\n",
      "Epoch 185, loss: 2.300445\n",
      "Epoch 186, loss: 2.300435\n",
      "Epoch 187, loss: 2.300425\n",
      "Epoch 188, loss: 2.300414\n",
      "Epoch 189, loss: 2.300404\n",
      "Epoch 190, loss: 2.300394\n",
      "Epoch 191, loss: 2.300384\n",
      "Epoch 192, loss: 2.300374\n",
      "Epoch 193, loss: 2.300364\n",
      "Epoch 194, loss: 2.300354\n",
      "Epoch 195, loss: 2.300344\n",
      "Epoch 196, loss: 2.300334\n",
      "Epoch 197, loss: 2.300324\n",
      "Epoch 198, loss: 2.300314\n",
      "Epoch 199, loss: 2.300304\n",
      "Epoch 0, loss: 2.302484\n",
      "Epoch 1, loss: 2.302473\n",
      "Epoch 2, loss: 2.302462\n",
      "Epoch 3, loss: 2.302452\n",
      "Epoch 4, loss: 2.302441\n",
      "Epoch 5, loss: 2.302430\n",
      "Epoch 6, loss: 2.302419\n",
      "Epoch 7, loss: 2.302408\n",
      "Epoch 8, loss: 2.302398\n",
      "Epoch 9, loss: 2.302387\n",
      "Epoch 10, loss: 2.302376\n",
      "Epoch 11, loss: 2.302365\n",
      "Epoch 12, loss: 2.302355\n",
      "Epoch 13, loss: 2.302344\n",
      "Epoch 14, loss: 2.302333\n",
      "Epoch 15, loss: 2.302323\n",
      "Epoch 16, loss: 2.302312\n",
      "Epoch 17, loss: 2.302301\n",
      "Epoch 18, loss: 2.302290\n",
      "Epoch 19, loss: 2.302280\n",
      "Epoch 20, loss: 2.302269\n",
      "Epoch 21, loss: 2.302258\n",
      "Epoch 22, loss: 2.302248\n",
      "Epoch 23, loss: 2.302237\n",
      "Epoch 24, loss: 2.302226\n",
      "Epoch 25, loss: 2.302216\n",
      "Epoch 26, loss: 2.302205\n",
      "Epoch 27, loss: 2.302194\n",
      "Epoch 28, loss: 2.302184\n",
      "Epoch 29, loss: 2.302173\n",
      "Epoch 30, loss: 2.302163\n",
      "Epoch 31, loss: 2.302152\n",
      "Epoch 32, loss: 2.302141\n",
      "Epoch 33, loss: 2.302131\n",
      "Epoch 34, loss: 2.302120\n",
      "Epoch 35, loss: 2.302110\n",
      "Epoch 36, loss: 2.302099\n",
      "Epoch 37, loss: 2.302088\n",
      "Epoch 38, loss: 2.302078\n",
      "Epoch 39, loss: 2.302067\n",
      "Epoch 40, loss: 2.302057\n",
      "Epoch 41, loss: 2.302046\n",
      "Epoch 42, loss: 2.302036\n",
      "Epoch 43, loss: 2.302025\n",
      "Epoch 44, loss: 2.302015\n",
      "Epoch 45, loss: 2.302004\n",
      "Epoch 46, loss: 2.301994\n",
      "Epoch 47, loss: 2.301983\n",
      "Epoch 48, loss: 2.301973\n",
      "Epoch 49, loss: 2.301962\n",
      "Epoch 50, loss: 2.301952\n",
      "Epoch 51, loss: 2.301941\n",
      "Epoch 52, loss: 2.301931\n",
      "Epoch 53, loss: 2.301920\n",
      "Epoch 54, loss: 2.301910\n",
      "Epoch 55, loss: 2.301899\n",
      "Epoch 56, loss: 2.301889\n",
      "Epoch 57, loss: 2.301879\n",
      "Epoch 58, loss: 2.301868\n",
      "Epoch 59, loss: 2.301858\n",
      "Epoch 60, loss: 2.301847\n",
      "Epoch 61, loss: 2.301837\n",
      "Epoch 62, loss: 2.301827\n",
      "Epoch 63, loss: 2.301816\n",
      "Epoch 64, loss: 2.301806\n",
      "Epoch 65, loss: 2.301795\n",
      "Epoch 66, loss: 2.301785\n",
      "Epoch 67, loss: 2.301775\n",
      "Epoch 68, loss: 2.301764\n",
      "Epoch 69, loss: 2.301754\n",
      "Epoch 70, loss: 2.301744\n",
      "Epoch 71, loss: 2.301733\n",
      "Epoch 72, loss: 2.301723\n",
      "Epoch 73, loss: 2.301713\n",
      "Epoch 74, loss: 2.301702\n",
      "Epoch 75, loss: 2.301692\n",
      "Epoch 76, loss: 2.301682\n",
      "Epoch 77, loss: 2.301671\n",
      "Epoch 78, loss: 2.301661\n",
      "Epoch 79, loss: 2.301651\n",
      "Epoch 80, loss: 2.301640\n",
      "Epoch 81, loss: 2.301630\n",
      "Epoch 82, loss: 2.301620\n",
      "Epoch 83, loss: 2.301610\n",
      "Epoch 84, loss: 2.301599\n",
      "Epoch 85, loss: 2.301589\n",
      "Epoch 86, loss: 2.301579\n",
      "Epoch 87, loss: 2.301569\n",
      "Epoch 88, loss: 2.301558\n",
      "Epoch 89, loss: 2.301548\n",
      "Epoch 90, loss: 2.301538\n",
      "Epoch 91, loss: 2.301528\n",
      "Epoch 92, loss: 2.301517\n",
      "Epoch 93, loss: 2.301507\n",
      "Epoch 94, loss: 2.301497\n",
      "Epoch 95, loss: 2.301487\n",
      "Epoch 96, loss: 2.301477\n",
      "Epoch 97, loss: 2.301466\n",
      "Epoch 98, loss: 2.301456\n",
      "Epoch 99, loss: 2.301446\n",
      "Epoch 100, loss: 2.301436\n",
      "Epoch 101, loss: 2.301426\n",
      "Epoch 102, loss: 2.301415\n",
      "Epoch 103, loss: 2.301405\n",
      "Epoch 104, loss: 2.301395\n",
      "Epoch 105, loss: 2.301385\n",
      "Epoch 106, loss: 2.301375\n",
      "Epoch 107, loss: 2.301365\n",
      "Epoch 108, loss: 2.301355\n",
      "Epoch 109, loss: 2.301344\n",
      "Epoch 110, loss: 2.301334\n",
      "Epoch 111, loss: 2.301324\n",
      "Epoch 112, loss: 2.301314\n",
      "Epoch 113, loss: 2.301304\n",
      "Epoch 114, loss: 2.301294\n",
      "Epoch 115, loss: 2.301284\n",
      "Epoch 116, loss: 2.301274\n",
      "Epoch 117, loss: 2.301264\n",
      "Epoch 118, loss: 2.301254\n",
      "Epoch 119, loss: 2.301243\n",
      "Epoch 120, loss: 2.301233\n",
      "Epoch 121, loss: 2.301223\n",
      "Epoch 122, loss: 2.301213\n",
      "Epoch 123, loss: 2.301203\n",
      "Epoch 124, loss: 2.301193\n",
      "Epoch 125, loss: 2.301183\n",
      "Epoch 126, loss: 2.301173\n",
      "Epoch 127, loss: 2.301163\n",
      "Epoch 128, loss: 2.301153\n",
      "Epoch 129, loss: 2.301143\n",
      "Epoch 130, loss: 2.301133\n",
      "Epoch 131, loss: 2.301123\n",
      "Epoch 132, loss: 2.301113\n",
      "Epoch 133, loss: 2.301103\n",
      "Epoch 134, loss: 2.301093\n",
      "Epoch 135, loss: 2.301083\n",
      "Epoch 136, loss: 2.301073\n",
      "Epoch 137, loss: 2.301063\n",
      "Epoch 138, loss: 2.301053\n",
      "Epoch 139, loss: 2.301043\n",
      "Epoch 140, loss: 2.301033\n",
      "Epoch 141, loss: 2.301023\n",
      "Epoch 142, loss: 2.301013\n",
      "Epoch 143, loss: 2.301003\n",
      "Epoch 144, loss: 2.300993\n",
      "Epoch 145, loss: 2.300983\n",
      "Epoch 146, loss: 2.300973\n",
      "Epoch 147, loss: 2.300963\n",
      "Epoch 148, loss: 2.300953\n",
      "Epoch 149, loss: 2.300943\n",
      "Epoch 150, loss: 2.300934\n",
      "Epoch 151, loss: 2.300924\n",
      "Epoch 152, loss: 2.300914\n",
      "Epoch 153, loss: 2.300904\n",
      "Epoch 154, loss: 2.300894\n",
      "Epoch 155, loss: 2.300884\n",
      "Epoch 156, loss: 2.300874\n",
      "Epoch 157, loss: 2.300864\n",
      "Epoch 158, loss: 2.300854\n",
      "Epoch 159, loss: 2.300844\n",
      "Epoch 160, loss: 2.300834\n",
      "Epoch 161, loss: 2.300825\n",
      "Epoch 162, loss: 2.300815\n",
      "Epoch 163, loss: 2.300805\n",
      "Epoch 164, loss: 2.300795\n",
      "Epoch 165, loss: 2.300785\n",
      "Epoch 166, loss: 2.300775\n",
      "Epoch 167, loss: 2.300765\n",
      "Epoch 168, loss: 2.300756\n",
      "Epoch 169, loss: 2.300746\n",
      "Epoch 170, loss: 2.300736\n",
      "Epoch 171, loss: 2.300726\n",
      "Epoch 172, loss: 2.300716\n",
      "Epoch 173, loss: 2.300706\n",
      "Epoch 174, loss: 2.300697\n",
      "Epoch 175, loss: 2.300687\n",
      "Epoch 176, loss: 2.300677\n",
      "Epoch 177, loss: 2.300667\n",
      "Epoch 178, loss: 2.300657\n",
      "Epoch 179, loss: 2.300648\n",
      "Epoch 180, loss: 2.300638\n",
      "Epoch 181, loss: 2.300628\n",
      "Epoch 182, loss: 2.300618\n",
      "Epoch 183, loss: 2.300608\n",
      "Epoch 184, loss: 2.300599\n",
      "Epoch 185, loss: 2.300589\n",
      "Epoch 186, loss: 2.300579\n",
      "Epoch 187, loss: 2.300569\n",
      "Epoch 188, loss: 2.300559\n",
      "Epoch 189, loss: 2.300550\n",
      "Epoch 190, loss: 2.300540\n",
      "Epoch 191, loss: 2.300530\n",
      "Epoch 192, loss: 2.300520\n",
      "Epoch 193, loss: 2.300511\n",
      "Epoch 194, loss: 2.300501\n",
      "Epoch 195, loss: 2.300491\n",
      "Epoch 196, loss: 2.300482\n",
      "Epoch 197, loss: 2.300472\n",
      "Epoch 198, loss: 2.300462\n",
      "Epoch 199, loss: 2.300452\n",
      "Epoch 0, loss: 2.302669\n",
      "Epoch 1, loss: 2.302658\n",
      "Epoch 2, loss: 2.302646\n",
      "Epoch 3, loss: 2.302635\n",
      "Epoch 4, loss: 2.302624\n",
      "Epoch 5, loss: 2.302612\n",
      "Epoch 6, loss: 2.302601\n",
      "Epoch 7, loss: 2.302590\n",
      "Epoch 8, loss: 2.302578\n",
      "Epoch 9, loss: 2.302567\n",
      "Epoch 10, loss: 2.302556\n",
      "Epoch 11, loss: 2.302545\n",
      "Epoch 12, loss: 2.302533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 2.302522\n",
      "Epoch 14, loss: 2.302511\n",
      "Epoch 15, loss: 2.302500\n",
      "Epoch 16, loss: 2.302489\n",
      "Epoch 17, loss: 2.302477\n",
      "Epoch 18, loss: 2.302466\n",
      "Epoch 19, loss: 2.302455\n",
      "Epoch 20, loss: 2.302444\n",
      "Epoch 21, loss: 2.302433\n",
      "Epoch 22, loss: 2.302421\n",
      "Epoch 23, loss: 2.302410\n",
      "Epoch 24, loss: 2.302399\n",
      "Epoch 25, loss: 2.302388\n",
      "Epoch 26, loss: 2.302377\n",
      "Epoch 27, loss: 2.302366\n",
      "Epoch 28, loss: 2.302355\n",
      "Epoch 29, loss: 2.302344\n",
      "Epoch 30, loss: 2.302333\n",
      "Epoch 31, loss: 2.302322\n",
      "Epoch 32, loss: 2.302311\n",
      "Epoch 33, loss: 2.302299\n",
      "Epoch 34, loss: 2.302288\n",
      "Epoch 35, loss: 2.302277\n",
      "Epoch 36, loss: 2.302266\n",
      "Epoch 37, loss: 2.302255\n",
      "Epoch 38, loss: 2.302244\n",
      "Epoch 39, loss: 2.302233\n",
      "Epoch 40, loss: 2.302222\n",
      "Epoch 41, loss: 2.302211\n",
      "Epoch 42, loss: 2.302200\n",
      "Epoch 43, loss: 2.302189\n",
      "Epoch 44, loss: 2.302179\n",
      "Epoch 45, loss: 2.302168\n",
      "Epoch 46, loss: 2.302157\n",
      "Epoch 47, loss: 2.302146\n",
      "Epoch 48, loss: 2.302135\n",
      "Epoch 49, loss: 2.302124\n",
      "Epoch 50, loss: 2.302113\n",
      "Epoch 51, loss: 2.302102\n",
      "Epoch 52, loss: 2.302091\n",
      "Epoch 53, loss: 2.302080\n",
      "Epoch 54, loss: 2.302069\n",
      "Epoch 55, loss: 2.302059\n",
      "Epoch 56, loss: 2.302048\n",
      "Epoch 57, loss: 2.302037\n",
      "Epoch 58, loss: 2.302026\n",
      "Epoch 59, loss: 2.302015\n",
      "Epoch 60, loss: 2.302004\n",
      "Epoch 61, loss: 2.301994\n",
      "Epoch 62, loss: 2.301983\n",
      "Epoch 63, loss: 2.301972\n",
      "Epoch 64, loss: 2.301961\n",
      "Epoch 65, loss: 2.301950\n",
      "Epoch 66, loss: 2.301940\n",
      "Epoch 67, loss: 2.301929\n",
      "Epoch 68, loss: 2.301918\n",
      "Epoch 69, loss: 2.301907\n",
      "Epoch 70, loss: 2.301897\n",
      "Epoch 71, loss: 2.301886\n",
      "Epoch 72, loss: 2.301875\n",
      "Epoch 73, loss: 2.301864\n",
      "Epoch 74, loss: 2.301854\n",
      "Epoch 75, loss: 2.301843\n",
      "Epoch 76, loss: 2.301832\n",
      "Epoch 77, loss: 2.301822\n",
      "Epoch 78, loss: 2.301811\n",
      "Epoch 79, loss: 2.301800\n",
      "Epoch 80, loss: 2.301790\n",
      "Epoch 81, loss: 2.301779\n",
      "Epoch 82, loss: 2.301768\n",
      "Epoch 83, loss: 2.301758\n",
      "Epoch 84, loss: 2.301747\n",
      "Epoch 85, loss: 2.301736\n",
      "Epoch 86, loss: 2.301726\n",
      "Epoch 87, loss: 2.301715\n",
      "Epoch 88, loss: 2.301704\n",
      "Epoch 89, loss: 2.301694\n",
      "Epoch 90, loss: 2.301683\n",
      "Epoch 91, loss: 2.301673\n",
      "Epoch 92, loss: 2.301662\n",
      "Epoch 93, loss: 2.301652\n",
      "Epoch 94, loss: 2.301641\n",
      "Epoch 95, loss: 2.301630\n",
      "Epoch 96, loss: 2.301620\n",
      "Epoch 97, loss: 2.301609\n",
      "Epoch 98, loss: 2.301599\n",
      "Epoch 99, loss: 2.301588\n",
      "Epoch 100, loss: 2.301578\n",
      "Epoch 101, loss: 2.301567\n",
      "Epoch 102, loss: 2.301557\n",
      "Epoch 103, loss: 2.301546\n",
      "Epoch 104, loss: 2.301536\n",
      "Epoch 105, loss: 2.301525\n",
      "Epoch 106, loss: 2.301515\n",
      "Epoch 107, loss: 2.301504\n",
      "Epoch 108, loss: 2.301494\n",
      "Epoch 109, loss: 2.301483\n",
      "Epoch 110, loss: 2.301473\n",
      "Epoch 111, loss: 2.301462\n",
      "Epoch 112, loss: 2.301452\n",
      "Epoch 113, loss: 2.301441\n",
      "Epoch 114, loss: 2.301431\n",
      "Epoch 115, loss: 2.301421\n",
      "Epoch 116, loss: 2.301410\n",
      "Epoch 117, loss: 2.301400\n",
      "Epoch 118, loss: 2.301389\n",
      "Epoch 119, loss: 2.301379\n",
      "Epoch 120, loss: 2.301369\n",
      "Epoch 121, loss: 2.301358\n",
      "Epoch 122, loss: 2.301348\n",
      "Epoch 123, loss: 2.301337\n",
      "Epoch 124, loss: 2.301327\n",
      "Epoch 125, loss: 2.301317\n",
      "Epoch 126, loss: 2.301306\n",
      "Epoch 127, loss: 2.301296\n",
      "Epoch 128, loss: 2.301286\n",
      "Epoch 129, loss: 2.301275\n",
      "Epoch 130, loss: 2.301265\n",
      "Epoch 131, loss: 2.301255\n",
      "Epoch 132, loss: 2.301244\n",
      "Epoch 133, loss: 2.301234\n",
      "Epoch 134, loss: 2.301224\n",
      "Epoch 135, loss: 2.301213\n",
      "Epoch 136, loss: 2.301203\n",
      "Epoch 137, loss: 2.301193\n",
      "Epoch 138, loss: 2.301183\n",
      "Epoch 139, loss: 2.301172\n",
      "Epoch 140, loss: 2.301162\n",
      "Epoch 141, loss: 2.301152\n",
      "Epoch 142, loss: 2.301141\n",
      "Epoch 143, loss: 2.301131\n",
      "Epoch 144, loss: 2.301121\n",
      "Epoch 145, loss: 2.301111\n",
      "Epoch 146, loss: 2.301101\n",
      "Epoch 147, loss: 2.301090\n",
      "Epoch 148, loss: 2.301080\n",
      "Epoch 149, loss: 2.301070\n",
      "Epoch 150, loss: 2.301060\n",
      "Epoch 151, loss: 2.301049\n",
      "Epoch 152, loss: 2.301039\n",
      "Epoch 153, loss: 2.301029\n",
      "Epoch 154, loss: 2.301019\n",
      "Epoch 155, loss: 2.301009\n",
      "Epoch 156, loss: 2.300998\n",
      "Epoch 157, loss: 2.300988\n",
      "Epoch 158, loss: 2.300978\n",
      "Epoch 159, loss: 2.300968\n",
      "Epoch 160, loss: 2.300958\n",
      "Epoch 161, loss: 2.300948\n",
      "Epoch 162, loss: 2.300937\n",
      "Epoch 163, loss: 2.300927\n",
      "Epoch 164, loss: 2.300917\n",
      "Epoch 165, loss: 2.300907\n",
      "Epoch 166, loss: 2.300897\n",
      "Epoch 167, loss: 2.300887\n",
      "Epoch 168, loss: 2.300877\n",
      "Epoch 169, loss: 2.300867\n",
      "Epoch 170, loss: 2.300856\n",
      "Epoch 171, loss: 2.300846\n",
      "Epoch 172, loss: 2.300836\n",
      "Epoch 173, loss: 2.300826\n",
      "Epoch 174, loss: 2.300816\n",
      "Epoch 175, loss: 2.300806\n",
      "Epoch 176, loss: 2.300796\n",
      "Epoch 177, loss: 2.300786\n",
      "Epoch 178, loss: 2.300776\n",
      "Epoch 179, loss: 2.300766\n",
      "Epoch 180, loss: 2.300756\n",
      "Epoch 181, loss: 2.300746\n",
      "Epoch 182, loss: 2.300736\n",
      "Epoch 183, loss: 2.300726\n",
      "Epoch 184, loss: 2.300716\n",
      "Epoch 185, loss: 2.300706\n",
      "Epoch 186, loss: 2.300696\n",
      "Epoch 187, loss: 2.300686\n",
      "Epoch 188, loss: 2.300675\n",
      "Epoch 189, loss: 2.300665\n",
      "Epoch 190, loss: 2.300655\n",
      "Epoch 191, loss: 2.300645\n",
      "Epoch 192, loss: 2.300635\n",
      "Epoch 193, loss: 2.300626\n",
      "Epoch 194, loss: 2.300616\n",
      "Epoch 195, loss: 2.300606\n",
      "Epoch 196, loss: 2.300596\n",
      "Epoch 197, loss: 2.300586\n",
      "Epoch 198, loss: 2.300576\n",
      "Epoch 199, loss: 2.300566\n",
      "best validation accuracy achieved: 0.229000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y,\n",
    "                       epochs=num_epochs,\n",
    "                       learning_rate=lr,\n",
    "                       batch_size=batch_size,\n",
    "                       reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy == None:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.196000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
